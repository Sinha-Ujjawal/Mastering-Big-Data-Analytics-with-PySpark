{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pyspark-udemy",
   "display_name": "pyspark-udemy"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Fixing issues in our data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Hands-on-2\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "source": [
    "### Loading ratings dataset using proper arguments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\n",
    "        path=\"../../data-sets/ml-latest/ratings.csv\",\n",
    "        encoding=\"UTF-8\",\n",
    "        sep=\",\",\n",
    "        quote='\"',\n",
    "        schema=\"userId INT, movieId INT, rating DOUBLE, timestamp INT\",\n",
    "        header=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+------+----------+\n|userId|movieId|rating| timestamp|\n+------+-------+------+----------+\n|     1|    307|   3.5|1256677221|\n|     1|    481|   3.5|1256677456|\n|     1|   1091|   1.5|1256677471|\n|     1|   1257|   4.5|1256677460|\n|     1|   1449|   4.5|1256677264|\n+------+-------+------+----------+\nonly showing top 5 rows\n\nroot\n |-- userId: integer (nullable = true)\n |-- movieId: integer (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_ratings.show(n=5)\n",
    "df_ratings.printSchema()"
   ]
  },
  {
   "source": [
    "### pyspark.sql.functions module contain alot of functions that can be used for operating on our data, eg- functions.from_unixtime, functions.to_timestamp etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+------+--------------+\n|userId|movieId|rating|timestamp_unix|\n+------+-------+------+--------------+\n|     1|    307|   3.5|    1256677221|\n|     1|    481|   3.5|    1256677456|\n|     1|   1091|   1.5|    1256677471|\n|     1|   1257|   4.5|    1256677460|\n|     1|   1449|   4.5|    1256677264|\n+------+-------+------+--------------+\nonly showing top 5 rows\n\nroot\n |-- userId: integer (nullable = true)\n |-- movieId: integer (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp_unix: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# from the documentation (README.txt) of ml-latest dataset, we can see that the timestamp column has values in the unix posix format, i.e; the number of seconds from 1st July 1970. We will need to change it to spark timestamp for proper analysis\n",
    "\n",
    "# first we are going to rename the timestamp column to timestamp_unix\n",
    "df_ratings_renamed = df_ratings.withColumnRenamed(\"timestamp\", \"timestamp_unix\")\n",
    "\n",
    "df_ratings_renamed.show(n=5)\n",
    "df_ratings_renamed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+------+--------------+-------------------+\n",
      "|userId|movieId|rating|timestamp_unix|          timestamp|\n",
      "+------+-------+------+--------------+-------------------+\n",
      "|     1|    307|   3.5|    1256677221|2009-10-28 02:30:21|\n",
      "|     1|    481|   3.5|    1256677456|2009-10-28 02:34:16|\n",
      "|     1|   1091|   1.5|    1256677471|2009-10-28 02:34:31|\n",
      "|     1|   1257|   4.5|    1256677460|2009-10-28 02:34:20|\n",
      "|     1|   1449|   4.5|    1256677264|2009-10-28 02:31:04|\n",
      "+------+-------+------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp_unix: integer (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now we are going to create a new column called timestamp from applying f.from_unixtime to timestamp_unix. This will give string representation of timestamp_unix\n",
    "df_ratings_with_timestamp_as_string = (\n",
    "    df_ratings_renamed\n",
    "    .withColumn(\"timestamp\", f.from_unixtime(\"timestamp_unix\"))\n",
    ")\n",
    "\n",
    "df_ratings_with_timestamp_as_string.show(n=5)\n",
    "df_ratings_with_timestamp_as_string.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+------+--------------+-------------------+\n|userId|movieId|rating|timestamp_unix|          timestamp|\n+------+-------+------+--------------+-------------------+\n|     1|    307|   3.5|    1256677221|2009-10-28 02:30:21|\n|     1|    481|   3.5|    1256677456|2009-10-28 02:34:16|\n|     1|   1091|   1.5|    1256677471|2009-10-28 02:34:31|\n|     1|   1257|   4.5|    1256677460|2009-10-28 02:34:20|\n|     1|   1449|   4.5|    1256677264|2009-10-28 02:31:04|\n+------+-------+------+--------------+-------------------+\nonly showing top 5 rows\n\nroot\n |-- userId: integer (nullable = true)\n |-- movieId: integer (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp_unix: integer (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# now we are going to create a new column called timestamp from applying f.to_timestamp to timestamp (essentially replacing the timestamp column). This will give timestamp representation of timestamp column\n",
    "df_ratings_with_timestamp_as_timestamp = (\n",
    "    df_ratings_with_timestamp_as_string\n",
    "    .withColumn(\"timestamp\", f.to_timestamp(\"timestamp\"))\n",
    ")\n",
    "\n",
    "df_ratings_with_timestamp_as_timestamp.show(n=5)\n",
    "df_ratings_with_timestamp_as_timestamp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+------+--------------+-------------------+\n",
      "|userId|movieId|rating|timestamp_unix|          timestamp|\n",
      "+------+-------+------+--------------+-------------------+\n",
      "|     1|    307|   3.5|    1256677221|2009-10-28 02:30:21|\n",
      "|     1|    481|   3.5|    1256677456|2009-10-28 02:34:16|\n",
      "|     1|   1091|   1.5|    1256677471|2009-10-28 02:34:31|\n",
      "|     1|   1257|   4.5|    1256677460|2009-10-28 02:34:20|\n",
      "|     1|   1449|   4.5|    1256677264|2009-10-28 02:31:04|\n",
      "+------+-------+------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp_unix: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can chain all this operation in a single line, as spark dataframes are immutable. This makes the code more readable and easy to understand.\n",
    "\n",
    "df_final = (\n",
    "    df_ratings\n",
    "    .withColumnRenamed(\"timestamp\", \"timestamp_unix\")\n",
    "    .withColumn(\"timestamp\", f.from_unixtime(\"timestamp_unix\"))\n",
    "    .withColumn(\"timestamp\", f.to_timestamp(\"timestamp\"))\n",
    ")\n",
    "\n",
    "df_final.show(n=5)\n",
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+------+--------------+-------------------+\n|userId|movieId|rating|timestamp_unix|          timestamp|\n+------+-------+------+--------------+-------------------+\n|     1|    307|   3.5|    1256677221|2009-10-28 02:30:21|\n|     1|    481|   3.5|    1256677456|2009-10-28 02:34:16|\n|     1|   1091|   1.5|    1256677471|2009-10-28 02:34:31|\n|     1|   1257|   4.5|    1256677460|2009-10-28 02:34:20|\n|     1|   1449|   4.5|    1256677264|2009-10-28 02:31:04|\n+------+-------+------+--------------+-------------------+\nonly showing top 5 rows\n\nroot\n |-- userId: integer (nullable = true)\n |-- movieId: integer (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp_unix: integer (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# we can one step further and combine f.from_unix and f.to_timestamp together, as they are composable (thx to functional-programming)\n",
    "\n",
    "df_final = (\n",
    "    df_ratings\n",
    "    .withColumnRenamed(\"timestamp\", \"timestamp_unix\")\n",
    "    .withColumn(\"timestamp\", f.to_timestamp(f.from_unixtime(\"timestamp_unix\")))\n",
    ")\n",
    "\n",
    "df_final.show(n=5)\n",
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can go one more step to combine all this transformation as part of spark.read.csv\n",
    "\n",
    "df_ratings = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\n",
    "        path=\"../../data-sets/ml-latest/ratings.csv\",\n",
    "        encoding=\"UTF-8\",\n",
    "        sep=\",\",\n",
    "        quote='\"',\n",
    "        schema=\"userId INT, movieId INT, rating DOUBLE, timestamp INT\",\n",
    "        header=True,\n",
    "    )\n",
    "    .withColumnRenamed(\"timestamp\", \"timestamp_unix\")\n",
    "    .withColumn(\"timestamp\", f.to_timestamp(f.from_unixtime(\"timestamp_unix\")))\n",
    ")\n",
    "\n",
    "# This is going to make the code less verbose and easy to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+------+--------------+-------------------+\n|userId|movieId|rating|timestamp_unix|          timestamp|\n+------+-------+------+--------------+-------------------+\n|     1|    307|   3.5|    1256677221|2009-10-28 02:30:21|\n|     1|    481|   3.5|    1256677456|2009-10-28 02:34:16|\n|     1|   1091|   1.5|    1256677471|2009-10-28 02:34:31|\n|     1|   1257|   4.5|    1256677460|2009-10-28 02:34:20|\n|     1|   1449|   4.5|    1256677264|2009-10-28 02:31:04|\n+------+-------+------+--------------+-------------------+\nonly showing top 5 rows\n\nroot\n |-- userId: integer (nullable = true)\n |-- movieId: integer (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp_unix: integer (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_ratings.show(n=5)\n",
    "df_ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for our dataset timestamp_unix is useless, and let's say we have to remove it. We can do so by using .drop method of spark dataframes\n",
    "\n",
    "df_ratings = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\n",
    "        path=\"../../data-sets/ml-latest/ratings.csv\",\n",
    "        encoding=\"UTF-8\",\n",
    "        sep=\",\",\n",
    "        quote='\"',\n",
    "        schema=\"userId INT, movieId INT, rating DOUBLE, timestamp INT\",\n",
    "        header=True,\n",
    "    )\n",
    "    .withColumnRenamed(\"timestamp\", \"timestamp_unix\")\n",
    "    .withColumn(\"timestamp\", f.to_timestamp(f.from_unixtime(\"timestamp_unix\")))\n",
    "    # again we can keep it as part of the spark.read.csv chain\n",
    "    .drop(\"timestamp_unix\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+------+-------------------+\n|userId|movieId|rating|          timestamp|\n+------+-------+------+-------------------+\n|     1|    307|   3.5|2009-10-28 02:30:21|\n|     1|    481|   3.5|2009-10-28 02:34:16|\n|     1|   1091|   1.5|2009-10-28 02:34:31|\n|     1|   1257|   4.5|2009-10-28 02:34:20|\n|     1|   1449|   4.5|2009-10-28 02:31:04|\n+------+-------+------+-------------------+\nonly showing top 5 rows\n\nroot\n |-- userId: integer (nullable = true)\n |-- movieId: integer (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_ratings.show(n=5)\n",
    "df_ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one thing to note here is that, if the column does not exist in the dataframe. It will not do anything, eg-\n",
    "\n",
    "df_ratings = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\n",
    "        path=\"../../data-sets/ml-latest/ratings.csv\",\n",
    "        encoding=\"UTF-8\",\n",
    "        sep=\",\",\n",
    "        quote='\"',\n",
    "        schema=\"userId INT, movieId INT, rating DOUBLE, timestamp INT\",\n",
    "        header=True,\n",
    "    )\n",
    "    .withColumnRenamed(\"timestamp\", \"timestamp_unix\")\n",
    "    .withColumn(\"timestamp\", f.to_timestamp(f.from_unixtime(\"timestamp_unix\")))\n",
    "    # foobar does not exist, nothing breaks\n",
    "    .drop(\"timestamp_unix\", \"foobar\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+------+-------------------+\n|userId|movieId|rating|          timestamp|\n+------+-------+------+-------------------+\n|     1|    307|   3.5|2009-10-28 02:30:21|\n|     1|    481|   3.5|2009-10-28 02:34:16|\n|     1|   1091|   1.5|2009-10-28 02:34:31|\n|     1|   1257|   4.5|2009-10-28 02:34:20|\n|     1|   1449|   4.5|2009-10-28 02:31:04|\n+------+-------+------+-------------------+\nonly showing top 5 rows\n\nroot\n |-- userId: integer (nullable = true)\n |-- movieId: integer (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_ratings.show(n=5)\n",
    "df_ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}