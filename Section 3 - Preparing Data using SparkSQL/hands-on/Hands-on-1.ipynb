{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pyspark-udemy",
   "display_name": "pyspark-udemy"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Hands-on-1\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\"../../data-sets/ml-latest/ratings.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+------+----------+\n|   _c0|    _c1|   _c2|       _c3|\n+------+-------+------+----------+\n|userId|movieId|rating| timestamp|\n|     1|    307|   3.5|1256677221|\n|     1|    481|   3.5|1256677456|\n|     1|   1091|   1.5|1256677471|\n|     1|   1257|   4.5|1256677460|\n+------+-------+------+----------+\nonly showing top 5 rows\n\nroot\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_ratings.show(n=5) # you can provide n=5 for showing only the top 5 rows, as spark is lazy it will only load the top n rows to show. Also you can specify truncate=False to disable truncation of the data if it's too big to fit in the screen\n",
    "df_ratings.printSchema()"
   ]
  },
  {
   "source": [
    "### As you can see, by default pyspark does not infer the schema of the data, and consider all of them as string. Also, by default it does not infer the header in the .csv files. For that we will need to pass some argument to .read.csv method of spark object"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\n",
    "        path=\"../../data-sets/ml-latest/ratings.csv\",\n",
    "        sep=\",\", # most .csv files are comma \",\" seperated. You can specify you own seperator symbol here\n",
    "        header=True, # for inferring the header as 1st row,\n",
    "        inferSchema=True, # for inferring the schema (i.e; the datatypes) of the dataframe\n",
    "        quote='\"', # most .csv files escape commas in data using \" character, for anything else you can specify the value here\n",
    "        encoding=\"UTF-8\", # most .csv files are UTF-8 encoded. ISO-8859-1 is also very common\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+------+----------+\n|userId|movieId|rating| timestamp|\n+------+-------+------+----------+\n|     1|    307|   3.5|1256677221|\n|     1|    481|   3.5|1256677456|\n|     1|   1091|   1.5|1256677471|\n|     1|   1257|   4.5|1256677460|\n|     1|   1449|   4.5|1256677264|\n+------+-------+------+----------+\nonly showing top 5 rows\n\nroot\n |-- userId: integer (nullable = true)\n |-- movieId: integer (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_ratings.show(n=5)\n",
    "df_ratings.printSchema()"
   ]
  },
  {
   "source": [
    "### As you can see by providing proper arguments we were able to parse the csv file in the desired format. Note that inferSchema should only be used for adhoc analysis, in production we should always enforce schema on our files for type-safety."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\n",
    "        path=\"../../data-sets/ml-latest/ratings.csv\",\n",
    "        sep=\",\", # most .csv files are comma \",\" seperated. You can specify you own seperator symbol here\n",
    "        header=True, # for inferring the header as 1st row,\n",
    "        # inferSchema=True, # for inferring the schema (i.e; the datatypes) of the dataframe\n",
    "        schema=\"userId INT, movieId INT, rating DOUBLE, timestamp INT\",\n",
    "        quote='\"', # most .csv files escape commas in data using \" character, for anything else you can specify the value here\n",
    "        encoding=\"UTF-8\", # most .csv files are UTF-8 encoded. ISO-8859-1 is also very common\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+------+----------+\n|userId|movieId|rating| timestamp|\n+------+-------+------+----------+\n|     1|    307|   3.5|1256677221|\n|     1|    481|   3.5|1256677456|\n|     1|   1091|   1.5|1256677471|\n|     1|   1257|   4.5|1256677460|\n|     1|   1449|   4.5|1256677264|\n+------+-------+------+----------+\nonly showing top 5 rows\n\nroot\n |-- userId: integer (nullable = true)\n |-- movieId: integer (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_ratings.show(n=5)\n",
    "df_ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}