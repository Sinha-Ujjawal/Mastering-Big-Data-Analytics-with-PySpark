{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pyspark-udemy",
   "display_name": "pyspark-udemy"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as f\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Hands-on-1\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\n",
    "        path=\"../../data-sets/ml-latest-small/ratings.csv\", # Using small to use less memory, that only fit in my memory (32 GB RAM, 16 cores)\n",
    "        encoding=\"UTF-8\",\n",
    "        header=True,\n",
    "        sep=\",\",\n",
    "        quote='\"',\n",
    "        schema=\"userId INT, movieId INT, rating DOUBLE\", # Notice, have dropped the timestamp column as not needed\n",
    "    )\n",
    "    .cache() # for speeding up\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+------+\n|userId|movieId|rating|\n+------+-------+------+\n|1     |1      |4.0   |\n|1     |3      |4.0   |\n|1     |6      |4.0   |\n|1     |47     |5.0   |\n|1     |50     |5.0   |\n+------+-------+------+\nonly showing top 5 rows\n\nroot\n |-- userId: integer (nullable = true)\n |-- movieId: integer (nullable = true)\n |-- rating: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_ratings.show(n=5, truncate=False)\n",
    "df_ratings.printSchema()"
   ]
  },
  {
   "source": [
    "### Let's see the summary of ratings dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+------------------+----------------+------------------+\n|summary|            userId|         movieId|            rating|\n+-------+------------------+----------------+------------------+\n|  count|            100836|          100836|            100836|\n|   mean|326.12756356856676|19435.2957177992| 3.501556983616962|\n| stddev| 182.6184914635004|35530.9871987003|1.0425292390606342|\n|    min|                 1|               1|               0.5|\n|    25%|               177|            1199|               3.0|\n|    50%|               325|            2991|               3.5|\n|    75%|               477|            8092|               4.0|\n|    max|               610|          193609|               5.0|\n+-------+------------------+----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_ratings.summary().show()"
   ]
  },
  {
   "source": [
    "```python\n",
    "class pyspark.ml.recommendation.ALS(\n",
    "    rank=10,\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    numUserBlocks=10,\n",
    "    numItemBlocks=10,\n",
    "    implicitPrefs=False,\n",
    "    alpha=1.0,\n",
    "    userCol='user',\n",
    "    itemCol='item',\n",
    "    seed=None,\n",
    "    ratingCol='rating',\n",
    "    nonnegative=False,\n",
    "    checkpointInterval=10,\n",
    "    intermediateStorageLevel='MEMORY_AND_DISK',\n",
    "    finalStorageLevel='MEMORY_AND_DISK',\n",
    "    coldStartStrategy='nan',\n",
    "    blockSize=4096,\n",
    ")\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Let's instantiate our model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = (\n",
    "    ALS(\n",
    "        userCol=\"userId\",\n",
    "        itemCol=\"movieId\",\n",
    "        ratingCol=\"rating\",\n",
    "    )\n",
    "    .fit(df_ratings)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.ml.recommendation.ALSModel"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "source": [
    "### Making prediction on df_ratings, using .transform method of class-\n",
    "```python\n",
    "pyspark.ml.recommendation.ALSModel\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(df_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[userId: int, movieId: int, rating: double, prediction: float]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# predictions is a dataframe with a prediction column\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+------+----------+\n|userId|movieId|rating|prediction|\n+------+-------+------+----------+\n|191   |148    |5.0   |4.9106784 |\n|133   |471    |4.0   |3.1113226 |\n|597   |471    |2.0   |3.9401305 |\n|385   |471    |4.0   |3.0269897 |\n|436   |471    |3.0   |3.4239593 |\n|602   |471    |4.0   |3.4478495 |\n|91    |471    |1.0   |2.5758777 |\n|409   |471    |3.0   |3.6621046 |\n|372   |471    |3.0   |2.969416  |\n|599   |471    |2.5   |2.6618292 |\n|603   |471    |4.0   |3.5522776 |\n|182   |471    |4.5   |3.7826753 |\n|218   |471    |4.0   |3.3839726 |\n|474   |471    |3.0   |3.478273  |\n|500   |471    |1.0   |2.1239042 |\n|57    |471    |3.0   |3.651469  |\n|462   |471    |2.5   |3.3151805 |\n|387   |471    |3.0   |2.952231  |\n|610   |471    |4.0   |3.4498801 |\n|217   |471    |2.0   |2.9447467 |\n|555   |471    |3.0   |3.1891325 |\n|176   |471    |5.0   |3.9437947 |\n|520   |471    |5.0   |3.7705052 |\n|136   |471    |4.0   |3.7829053 |\n|171   |471    |3.0   |3.9314106 |\n|273   |471    |5.0   |4.177548  |\n|448   |471    |4.0   |3.634994  |\n|312   |471    |4.0   |3.5596592 |\n|216   |471    |3.0   |3.6032655 |\n|411   |471    |4.0   |3.1179833 |\n+------+-------+------+----------+\nonly showing top 30 rows\n\n"
     ]
    }
   ],
   "source": [
    "predictions.show(n=30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}